"""
SEOä¼˜åŒ–å™¨
é’ˆå¯¹çŸ¥ä¹å¹³å°çš„æœç´¢å¼•æ“ä¼˜åŒ–
"""

import re
from typing import List, Dict, Any, Optional
from collections import Counter
import jieba  # ä¸­æ–‡åˆ†è¯


class SEOOptimizer:
    """SEOä¼˜åŒ–å™¨"""
    
    # åœç”¨è¯åˆ—è¡¨ï¼ˆå¸¸è§çš„æ— æ„ä¹‰è¯ï¼‰
    STOP_WORDS = {
        "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", 
        "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ",
        "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰",
        "çœ‹", "å¥½", "è‡ªå·±", "è¿™"
    }
    
    def __init__(self):
        """åˆå§‹åŒ–SEOä¼˜åŒ–å™¨"""
        pass
    
    def extract_keywords(
        self,
        text: str,
        top_k: int = 10,
        min_length: int = 2
    ) -> List[Dict[str, Any]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            top_k: è¿”å›å‰Kä¸ªå…³é”®è¯
            min_length: æœ€å°è¯é•¿
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«è¯å’Œæƒé‡
        """
        # ä¸­æ–‡åˆ†è¯
        words = jieba.cut(text)
        
        # è¿‡æ»¤
        filtered_words = [
            word for word in words
            if len(word) >= min_length and word not in self.STOP_WORDS
        ]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(filtered_words)
        
        # è®¡ç®—æƒé‡ï¼ˆTFï¼‰
        total = sum(word_counts.values())
        keywords = [
            {
                "word": word,
                "count": count,
                "weight": round(count / total, 4)
            }
            for word, count in word_counts.most_common(top_k)
        ]
        
        return keywords
    
    def optimize_keywords_layout(
        self,
        title: str,
        keywords: List[str],
        max_length: int = 50
    ) -> str:
        """
        ä¼˜åŒ–æ ‡é¢˜ä¸­çš„å…³é”®è¯å¸ƒå±€
        
        Args:
            title: åŸæ ‡é¢˜
            keywords: å…³é”®è¯åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            ä¼˜åŒ–åçš„æ ‡é¢˜
        """
        # 1. ç¡®ä¿ä¸»å…³é”®è¯åœ¨å‰20ä¸ªå­—å†…
        if keywords:
            main_keyword = keywords[0]
            
            # å¦‚æœä¸»å…³é”®è¯ä¸åœ¨å‰é¢ï¼Œå°è¯•å‰ç½®
            if main_keyword in title and title.find(main_keyword) > 20:
                title = title.replace(main_keyword, "", 1)
                title = f"{main_keyword}{title}"
        
        # 2. ç¡®ä¿åŒ…å«å°½å¯èƒ½å¤šçš„å…³é”®è¯
        for keyword in keywords[1:]:
            if keyword not in title:
                # å°è¯•åœ¨åˆé€‚ä½ç½®æ’å…¥
                if len(title) + len(keyword) + 1 <= max_length:
                    # ç®€å•ç­–ç•¥ï¼šåœ¨ä¸­é—´æ’å…¥
                    mid = len(title) // 2
                    title = title[:mid] + keyword + title[mid:]
        
        # 3. æ§åˆ¶é•¿åº¦
        if len(title) > max_length:
            title = title[:max_length - 3] + "..."
        
        return title
    
    def generate_long_tail_keywords(
        self,
        main_keyword: str,
        related_keywords: List[str]
    ) -> List[str]:
        """
        ç”Ÿæˆé•¿å°¾å…³é”®è¯
        
        Args:
            main_keyword: ä¸»å…³é”®è¯
            related_keywords: ç›¸å…³å…³é”®è¯
            
        Returns:
            é•¿å°¾å…³é”®è¯åˆ—è¡¨
        """
        long_tail = []
        
        # ç»„åˆç­–ç•¥
        patterns = [
            "{main} {related}",
            "å¦‚ä½• {main}",
            "{main} æ•™ç¨‹",
            "{main} æ–¹æ³•",
            "{main} æŠ€å·§",
            "{main} æŒ‡å—",
            "æœ€å¥½çš„ {main}",
            "{main} æ¨è",
        ]
        
        for related in related_keywords:
            for pattern in patterns:
                keyword = pattern.format(main=main_keyword, related=related)
                long_tail.append(keyword)
        
        return long_tail[:10]  # è¿”å›å‰10ä¸ª
    
    def generate_meta_description(
        self,
        content: str,
        keywords: Optional[List[str]] = None,
        max_length: int = 160
    ) -> str:
        """
        ç”ŸæˆSEOå…ƒæè¿°
        
        Args:
            content: æ–‡ç« å†…å®¹
            keywords: å…³é”®è¯åˆ—è¡¨
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            å…ƒæè¿°
        """
        # 1. æå–æ–‡ç« å¼€å¤´ï¼ˆé€šå¸¸æ˜¯æ‘˜è¦ï¼‰
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
        description = sentences[0] if sentences else content[:max_length]
        
        # 2. ç¡®ä¿åŒ…å«å…³é”®è¯
        if keywords:
            for keyword in keywords[:3]:  # å‰3ä¸ªå…³é”®è¯
                if keyword not in description:
                    # å°è¯•åœ¨åˆé€‚ä½ç½®æ’å…¥
                    if len(description) + len(keyword) + 1 <= max_length:
                        description = f"{keyword}ï¼š{description}"
                        break
        
        # 3. æ§åˆ¶é•¿åº¦
        if len(description) > max_length:
            description = description[:max_length - 3] + "..."
        
        return description
    
    def analyze_keyword_density(
        self,
        text: str,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """
        åˆ†æå…³é”®è¯å¯†åº¦
        
        Args:
            text: æ–‡æœ¬
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            å¯†åº¦åˆ†æç»“æœ
        """
        # æ€»å­—æ•°
        total_chars = len(text)
        
        # åˆ†ææ¯ä¸ªå…³é”®è¯
        density_data = {}
        for keyword in keywords:
            count = text.count(keyword)
            density = round((len(keyword) * count) / total_chars * 100, 2)
            
            # åˆ¤æ–­å¯†åº¦æ˜¯å¦åˆç†ï¼ˆ2-8%ä¸ºæœ€ä½³ï¼‰
            status = "æœ€ä½³" if 2 <= density <= 8 else "è¿‡ä½" if density < 2 else "è¿‡é«˜"
            
            density_data[keyword] = {
                "count": count,
                "density": density,
                "status": status
            }
        
        return density_data
    
    def suggest_internal_links(
        self,
        content: str,
        available_articles: List[Dict[str, str]]
    ) -> List[Dict[str, str]]:
        """
        å»ºè®®å†…éƒ¨é“¾æ¥
        
        Args:
            content: å½“å‰æ–‡ç« å†…å®¹
            available_articles: å¯ç”¨æ–‡ç« åˆ—è¡¨ [{"title": "...", "url": "..."}]
            
        Returns:
            æ¨èé“¾æ¥åˆ—è¡¨
        """
        suggestions = []
        
        # æå–å½“å‰æ–‡ç« å…³é”®è¯
        current_keywords = self.extract_keywords(content, top_k=20)
        current_words = {kw["word"] for kw in current_keywords}
        
        # åŒ¹é…ç›¸å…³æ–‡ç« 
        for article in available_articles:
            article_title = article.get("title", "")
            article_keywords = self.extract_keywords(article_title, top_k=10)
            article_words = {kw["word"] for kw in article_keywords}
            
            # è®¡ç®—ç›¸å…³åº¦ï¼ˆå…±åŒå…³é”®è¯æ•°é‡ï¼‰
            common_words = current_words & article_words
            relevance = len(common_words)
            
            if relevance > 0:
                suggestions.append({
                    "title": article_title,
                    "url": article.get("url", ""),
                    "relevance": relevance,
                    "common_keywords": list(common_words)
                })
        
        # æŒ‰ç›¸å…³åº¦æ’åº
        suggestions.sort(key=lambda x: x["relevance"], reverse=True)
        
        return suggestions[:5]  # è¿”å›å‰5ä¸ª
    
    def check_readability(self, text: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å¯è¯»æ€§
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å¯è¯»æ€§åˆ†æ
        """
        # ç»Ÿè®¡
        total_chars = len(text)
        sentences = len(re.split(r'[ã€‚ï¼ï¼Ÿ]', text))
        paragraphs = len(re.split(r'\n\n+', text.strip()))
        
        # è®¡ç®—æŒ‡æ ‡
        avg_sentence_length = total_chars / sentences if sentences > 0 else 0
        avg_paragraph_length = total_chars / paragraphs if paragraphs > 0 else 0
        
        # è¯„åˆ†
        score = 100
        issues = []
        
        # 1. å¥å­é•¿åº¦ï¼ˆå»ºè®®20-50å­—ï¼‰
        if avg_sentence_length > 60:
            score -= 20
            issues.append("å¥å­è¿‡é•¿ï¼Œå»ºè®®åˆ†æ®µ")
        elif avg_sentence_length < 15:
            score -= 10
            issues.append("å¥å­è¿‡çŸ­ï¼Œå¯é€‚å½“å¢åŠ æè¿°")
        
        # 2. æ®µè½é•¿åº¦ï¼ˆå»ºè®®100-300å­—ï¼‰
        if avg_paragraph_length > 400:
            score -= 20
            issues.append("æ®µè½è¿‡é•¿ï¼Œå»ºè®®å¢åŠ åˆ†æ®µ")
        
        # 3. æ€»é•¿åº¦
        if total_chars < 500:
            score -= 15
            issues.append("å†…å®¹è¿‡çŸ­ï¼Œå»ºè®®å¢åŠ åˆ°800å­—ä»¥ä¸Š")
        
        return {
            "score": max(0, score),
            "level": "ä¼˜ç§€" if score >= 80 else "è‰¯å¥½" if score >= 60 else "éœ€æ”¹è¿›",
            "total_chars": total_chars,
            "sentences": sentences,
            "paragraphs": paragraphs,
            "avg_sentence_length": round(avg_sentence_length, 1),
            "avg_paragraph_length": round(avg_paragraph_length, 1),
            "issues": issues
        }


# æµ‹è¯•ä»£ç 
def test_seo_optimizer():
    """æµ‹è¯•SEOä¼˜åŒ–å™¨"""
    print("="*60)
    print("ğŸ§ª æµ‹è¯•SEOä¼˜åŒ–å™¨")
    print("="*60)
    
    optimizer = SEOOptimizer()
    
    # æµ‹è¯•æ–‡æœ¬
    test_content = """
    Pythonæ˜¯ä¸€é—¨éå¸¸å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ï¼Œå®ƒåœ¨æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€Webå¼€å‘ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
    å­¦ä¹ Pythonéœ€è¦æŒæ¡åŸºç¡€è¯­æ³•ã€æ•°æ®ç»“æ„ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ç­‰æ ¸å¿ƒæ¦‚å¿µã€‚
    æœ¬æ–‡å°†ä»‹ç»Pythonå­¦ä¹ çš„å®Œæ•´è·¯å¾„ï¼Œå¸®åŠ©åˆå­¦è€…å¿«é€Ÿå…¥é—¨ã€‚
    æˆ‘ä»¬ä¼šä»ç¯å¢ƒæ­å»ºå¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°å®æˆ˜é¡¹ç›®å¼€å‘ã€‚
    """
    
    # æµ‹è¯•1ï¼šæå–å…³é”®è¯
    print("\n1ï¸âƒ£ æµ‹è¯•å…³é”®è¯æå–")
    keywords = optimizer.extract_keywords(test_content, top_k=5)
    for kw in keywords:
        print(f"   - {kw['word']}: æƒé‡ {kw['weight']}, å‡ºç° {kw['count']} æ¬¡")
    
    # æµ‹è¯•2ï¼šä¼˜åŒ–å…³é”®è¯å¸ƒå±€
    print("\n2ï¸âƒ£ æµ‹è¯•å…³é”®è¯å¸ƒå±€ä¼˜åŒ–")
    title = "ç¼–ç¨‹è¯­è¨€å­¦ä¹ æŒ‡å—"
    optimized_title = optimizer.optimize_keywords_layout(
        title, 
        ["Python", "å…¥é—¨", "å®æˆ˜"]
    )
    print(f"   åŸæ ‡é¢˜: {title}")
    print(f"   ä¼˜åŒ–å: {optimized_title}")
    
    # æµ‹è¯•3ï¼šç”Ÿæˆé•¿å°¾å…³é”®è¯
    print("\n3ï¸âƒ£ æµ‹è¯•é•¿å°¾å…³é”®è¯ç”Ÿæˆ")
    long_tail = optimizer.generate_long_tail_keywords(
        "Python",
        ["å…¥é—¨", "æ•™ç¨‹"]
    )
    print("   é•¿å°¾å…³é”®è¯:")
    for i, kw in enumerate(long_tail[:5], 1):
        print(f"   {i}. {kw}")
    
    # æµ‹è¯•4ï¼šç”Ÿæˆå…ƒæè¿°
    print("\n4ï¸âƒ£ æµ‹è¯•å…ƒæè¿°ç”Ÿæˆ")
    meta_desc = optimizer.generate_meta_description(
        test_content,
        keywords=["Python", "å­¦ä¹ ", "å…¥é—¨"]
    )
    print(f"   å…ƒæè¿°: {meta_desc}")
    
    # æµ‹è¯•5ï¼šå…³é”®è¯å¯†åº¦åˆ†æ
    print("\n5ï¸âƒ£ æµ‹è¯•å…³é”®è¯å¯†åº¦åˆ†æ")
    density = optimizer.analyze_keyword_density(
        test_content,
        ["Python", "å­¦ä¹ ", "ç¼–ç¨‹"]
    )
    for word, data in density.items():
        print(f"   - {word}: {data['density']}% ({data['status']}), å‡ºç°{data['count']}æ¬¡")
    
    # æµ‹è¯•6ï¼šå¯è¯»æ€§æ£€æŸ¥
    print("\n6ï¸âƒ£ æµ‹è¯•å¯è¯»æ€§æ£€æŸ¥")
    readability = optimizer.check_readability(test_content)
    print(f"   è¯„åˆ†: {readability['score']}/100 ({readability['level']})")
    print(f"   æ€»å­—æ•°: {readability['total_chars']}")
    print(f"   å¹³å‡å¥é•¿: {readability['avg_sentence_length']}å­—")
    if readability['issues']:
        print(f"   å»ºè®®: {', '.join(readability['issues'])}")
    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    test_seo_optimizer()


