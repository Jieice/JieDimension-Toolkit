"""
JieDimension Toolkit - AI Engine
æ™ºèƒ½AIè°ƒåº¦å¼•æ“ï¼Œæ”¯æŒæœ¬åœ°Ollama + å¤šäº‘ç«¯APIè½®æ›¿
Version: 1.5.0 - Day 16: æ·»åŠ AIè¾“å‡ºæ¸…ç†åŠŸèƒ½
Author: JieDimension Studio
"""

import asyncio
import time
import json
import logging
import os
import re
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from enum import Enum
import httpx

# Gemini APIæ”¯æŒ
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logger_temp = logging.getLogger(__name__)
    logger_temp.warning("âš ï¸ google-generativeaiæœªå®‰è£…ï¼ŒGeminiåŠŸèƒ½ä¸å¯ç”¨")

# Claude APIæ”¯æŒï¼ˆä½¿ç”¨httpxç›´æ¥è°ƒç”¨ï¼‰
CLAUDE_AVAILABLE = True  # Claudeä½¿ç”¨REST APIï¼Œæ— éœ€é¢å¤–åº“

# æ–‡å¿ƒä¸€è¨€APIæ”¯æŒï¼ˆä½¿ç”¨httpxç›´æ¥è°ƒç”¨ï¼‰
ERNIE_AVAILABLE = True  # æ–‡å¿ƒä¸€è¨€ä½¿ç”¨REST APIï¼Œæ— éœ€é¢å¤–åº“

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def clean_ai_output(text: str) -> str:
    """
    æ¸…ç†AIè¾“å‡ºå†…å®¹
    - å»é™¤deepseek-r1æ¨¡å‹çš„<think>...</think>æ€è€ƒè¿‡ç¨‹æ ‡ç­¾
    - å»é™¤å¤šä½™çš„ç©ºç™½
    - ä¿ç•™å®é™…è¾“å‡ºå†…å®¹
    
    Args:
        text: åŸå§‹AIè¾“å‡º
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # å»é™¤<think>...</think>æ ‡ç­¾åŠå…¶å†…å®¹
    # ä½¿ç”¨re.DOTALLè®©.åŒ¹é…æ¢è¡Œç¬¦
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # å»é™¤å¤šä½™çš„ç©ºç™½
    cleaned = cleaned.strip()
    
    # å»é™¤å¤šä¸ªè¿ç»­æ¢è¡Œï¼Œæœ€å¤šä¿ç•™ä¸¤ä¸ªæ¢è¡Œ
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    
    return cleaned


class TaskComplexity(Enum):
    """ä»»åŠ¡å¤æ‚åº¦æšä¸¾"""
    SIMPLE = 1      # ç®€å•ä»»åŠ¡ï¼ˆæ ‡é¢˜ä¼˜åŒ–ï¼‰-> æœ¬åœ°æ¨¡å‹
    MEDIUM = 2      # ä¸­ç­‰ä»»åŠ¡ï¼ˆæè¿°ç”Ÿæˆï¼‰-> æœ¬åœ°æ¨¡å‹ or å…è´¹API
    COMPLEX = 3     # å¤æ‚ä»»åŠ¡ï¼ˆå¤šè½®å¯¹è¯ï¼‰-> å…è´¹APIä¼˜å…ˆ
    ADVANCED = 4    # é«˜çº§ä»»åŠ¡ï¼ˆé•¿æ–‡æœ¬ç”Ÿæˆï¼‰-> é«˜çº§API


class AIProvider(Enum):
    """AIæä¾›å•†æšä¸¾"""
    OLLAMA = "ollama"
    GEMINI = "gemini"
    CLAUDE = "claude"      # Day 7æ–°å¢
    ERNIE = "ernie"        # Day 7æ–°å¢ - æ–‡å¿ƒä¸€è¨€
    COHERE = "cohere"      # é¢„ç•™
    DOUBAO = "doubao"      # é¢„ç•™


@dataclass
class AIConfig:
    """AIé…ç½®"""
    # Ollamaé…ç½®
    ollama_url: str = "http://localhost:11434"
    ollama_model: str = "deepseek-r1:1.5b"
    
    # Geminié…ç½®
    gemini_api_key: Optional[str] = None
    gemini_model: str = "gemini-1.5-flash"  # å…è´¹ç‰ˆ
    
    # Claudeé…ç½®ï¼ˆDay 7æ–°å¢ï¼‰
    claude_api_key: Optional[str] = None
    claude_model: str = "claude-3-5-sonnet-20241022"  # æœ€æ–°Sonnetæ¨¡å‹
    claude_base_url: str = "https://api.anthropic.com/v1/messages"
    
    # æ–‡å¿ƒä¸€è¨€é…ç½®ï¼ˆDay 7æ–°å¢ï¼‰
    ernie_api_key: Optional[str] = None
    ernie_secret_key: Optional[str] = None
    ernie_model: str = "ernie-4.0-8k"  # ERNIE 4.0
    ernie_base_url: str = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat"
    
    # APIé…ç½®
    max_retries: int = 3
    timeout: int = 30
    
    # è°ƒåº¦ç­–ç•¥
    prefer_local: bool = True  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆç®€å•ä»»åŠ¡ï¼‰
    fallback_enabled: bool = True  # å¯ç”¨é™çº§ç­–ç•¥
    use_cloud_for_complex: bool = True  # å¤æ‚ä»»åŠ¡ä½¿ç”¨äº‘ç«¯


@dataclass
class AIResponse:
    """AIå“åº”"""
    success: bool
    content: str
    provider: str
    model: str
    latency: float
    tokens: Optional[int] = None
    error: Optional[str] = None


class AIEngine:
    """
    AIæ™ºèƒ½è°ƒåº¦å¼•æ“
    
    åŠŸèƒ½ï¼š
    1. æœ¬åœ°Ollamaä¼˜å…ˆè°ƒç”¨ï¼ˆæ— é™åˆ¶ï¼Œæœ€å¿«ï¼‰
    2. å…è´¹APIè½®æ›¿ï¼ˆGemini, Cohere, è±†åŒ…ï¼‰
    3. ä»»åŠ¡å¤æ‚åº¦æ™ºèƒ½åˆ†çº§
    4. å¤±è´¥è‡ªåŠ¨é‡è¯•ä¸é™çº§
    """
    
    def __init__(self, config: Optional[AIConfig] = None):
        """åˆå§‹åŒ–AIå¼•æ“"""
        self.config = config or AIConfig()
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½APIå¯†é’¥ï¼ˆå¦‚æœæœªåœ¨configä¸­è®¾ç½®ï¼‰
        if not self.config.gemini_api_key:
            self.config.gemini_api_key = os.getenv('GEMINI_API_KEY')
        if not self.config.claude_api_key:
            self.config.claude_api_key = os.getenv('CLAUDE_API_KEY')
        if not self.config.ernie_api_key:
            self.config.ernie_api_key = os.getenv('ERNIE_API_KEY')
        if not self.config.ernie_secret_key:
            self.config.ernie_secret_key = os.getenv('ERNIE_SECRET_KEY')
        
        # åˆå§‹åŒ–Gemini
        self.gemini_model = None
        if GEMINI_AVAILABLE and self.config.gemini_api_key:
            try:
                genai.configure(api_key=self.config.gemini_api_key)
                self.gemini_model = genai.GenerativeModel(self.config.gemini_model)
                logger.info(f"âœ… Geminiå·²é…ç½®: {self.config.gemini_model}")
            except Exception as e:
                logger.warning(f"âš ï¸ Geminié…ç½®å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–Claudeï¼ˆREST APIï¼Œæ— éœ€ç‰¹æ®Šé…ç½®ï¼‰
        self.claude_available = CLAUDE_AVAILABLE and bool(self.config.claude_api_key)
        if self.claude_available:
            logger.info(f"âœ… Claudeå·²é…ç½®: {self.config.claude_model}")
        
        # åˆå§‹åŒ–æ–‡å¿ƒä¸€è¨€ï¼ˆéœ€è¦è·å–access_tokenï¼‰
        self.ernie_access_token = None
        self.ernie_available = False
        if ERNIE_AVAILABLE and self.config.ernie_api_key and self.config.ernie_secret_key:
            logger.info("ğŸ“ æ–‡å¿ƒä¸€è¨€é…ç½®å®Œæˆï¼Œå°†åœ¨é¦–æ¬¡è°ƒç”¨æ—¶è·å–access_token")
            self.ernie_available = True
        
        self.api_providers = []
        self.provider_stats = {}
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        for provider in AIProvider:
            self.provider_stats[provider.value] = {
                'total_calls': 0,
                'success_calls': 0,
                'failed_calls': 0,
                'total_latency': 0.0,
                'avg_latency': 0.0,
                'enabled': True
            }
        
        logger.info("ğŸš€ AIå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - Ollamaæ¨¡å‹: {self.config.ollama_model}")
        logger.info(f"   - Ollamaåœ°å€: {self.config.ollama_url}")
        logger.info(f"   - GeminiçŠ¶æ€: {'å¯ç”¨' if self.gemini_model else 'æœªé…ç½®'}")
        logger.info(f"   - ClaudeçŠ¶æ€: {'å¯ç”¨' if self.claude_available else 'æœªé…ç½®'}")
        logger.info(f"   - æ–‡å¿ƒä¸€è¨€çŠ¶æ€: {'å¯ç”¨' if self.ernie_available else 'æœªé…ç½®'}")
    
    async def test_ollama_connection(self) -> bool:
        """
        æµ‹è¯•Ollamaè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("ğŸ” æµ‹è¯•Ollamaè¿æ¥...")
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                # æµ‹è¯•OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ
                response = await client.get(f"{self.config.ollama_url}/api/tags")
                
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    model_names = [m['name'] for m in models]
                    
                    logger.info(f"âœ… Ollamaè¿æ¥æˆåŠŸï¼")
                    logger.info(f"   - å¯ç”¨æ¨¡å‹: {', '.join(model_names)}")
                    
                    # æ£€æŸ¥ç›®æ ‡æ¨¡å‹æ˜¯å¦å­˜åœ¨
                    if self.config.ollama_model in model_names:
                        logger.info(f"   - âœ“ æ‰¾åˆ°æ¨¡å‹: {self.config.ollama_model}")
                        return True
                    else:
                        logger.warning(f"   - âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹: {self.config.ollama_model}")
                        logger.warning(f"   - è¯·è¿è¡Œ: ollama pull {self.config.ollama_model}")
                        return False
                else:
                    logger.error(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                    return False
                    
        except httpx.ConnectError:
            logger.error("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡")
            logger.error("   - è¯·ç¡®è®¤Ollamaå·²å¯åŠ¨")
            logger.error("   - å¯åŠ¨å‘½ä»¤: ollama serve")
            return False
        except Exception as e:
            logger.error(f"âŒ Ollamaè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def _call_ollama(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> AIResponse:
        """
        è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            AIResponse: AIå“åº”
        """
        start_time = time.time()
        provider = AIProvider.OLLAMA.value
        
        try:
            logger.info(f"ğŸ“¤ è°ƒç”¨Ollama: {self.config.ollama_model}")
            
            # æ„å»ºè¯·æ±‚
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            payload = {
                "model": self.config.ollama_model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }
            
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.post(
                    f"{self.config.ollama_url}/api/chat",
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    raw_content = result.get('message', {}).get('content', '')
                    
                    # æ¸…ç†AIè¾“å‡ºï¼ˆå»é™¤<think>æ ‡ç­¾ç­‰ï¼‰
                    content = clean_ai_output(raw_content)
                    
                    latency = time.time() - start_time
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(provider, True, latency)
                    
                    logger.info(f"âœ… Ollamaå“åº”æˆåŠŸ (è€—æ—¶: {latency:.2f}s)")
                    
                    return AIResponse(
                        success=True,
                        content=content,
                        provider=provider,
                        model=self.config.ollama_model,
                        latency=latency,
                        tokens=result.get('eval_count')
                    )
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                    
        except Exception as e:
            latency = time.time() - start_time
            self._update_stats(provider, False, latency)
            
            logger.error(f"âŒ Ollamaè°ƒç”¨å¤±è´¥: {e}")
            
            return AIResponse(
                success=False,
                content="",
                provider=provider,
                model=self.config.ollama_model,
                latency=latency,
                error=str(e)
            )
    
    async def _call_gemini(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> AIResponse:
        """
        è°ƒç”¨Gemini API
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            AIResponse: AIå“åº”
        """
        start_time = time.time()
        provider = AIProvider.GEMINI.value
        
        try:
            if not self.gemini_model:
                raise Exception("Geminiæœªé…ç½®æˆ–ä¸å¯ç”¨")
            
            logger.info(f"ğŸ“¤ è°ƒç”¨Gemini: {self.config.gemini_model}")
            
            # æ„å»ºå®Œæ•´æç¤ºè¯
            full_prompt = prompt
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{prompt}"
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            generation_config = genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=2048,
            )
            
            # è°ƒç”¨Gemini API
            response = await asyncio.to_thread(
                self.gemini_model.generate_content,
                full_prompt,
                generation_config=generation_config
            )
            
            raw_content = response.text
            # æ¸…ç†AIè¾“å‡ºï¼ˆå»é™¤<think>æ ‡ç­¾ç­‰ï¼‰
            content = clean_ai_output(raw_content)
            
            latency = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_stats(provider, True, latency)
            
            logger.info(f"âœ… Geminiå“åº”æˆåŠŸ (è€—æ—¶: {latency:.2f}s)")
            
            return AIResponse(
                success=True,
                content=content,
                provider=provider,
                model=self.config.gemini_model,
                latency=latency,
                tokens=len(content.split())  # ç²—ç•¥ä¼°è®¡
            )
                    
        except Exception as e:
            latency = time.time() - start_time
            self._update_stats(provider, False, latency)
            
            logger.error(f"âŒ Geminiè°ƒç”¨å¤±è´¥: {e}")
            
            return AIResponse(
                success=False,
                content="",
                provider=provider,
                model=self.config.gemini_model,
                latency=latency,
                error=str(e)
            )
    
    async def _call_claude(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> AIResponse:
        """
        è°ƒç”¨Claude API
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            AIResponse: AIå“åº”
        """
        start_time = time.time()
        provider = AIProvider.CLAUDE.value
        
        try:
            if not self.claude_available:
                raise Exception("Claudeæœªé…ç½®æˆ–ä¸å¯ç”¨")
            
            logger.info(f"ğŸ“¤ è°ƒç”¨Claude: {self.config.claude_model}")
            
            # æ„å»ºæ¶ˆæ¯
            messages = [{"role": "user", "content": prompt}]
            
            # æ„å»ºè¯·æ±‚ä½“
            payload = {
                "model": self.config.claude_model,
                "max_tokens": 2048,
                "temperature": temperature,
                "messages": messages
            }
            
            # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if system_prompt:
                payload["system"] = system_prompt
            
            # è°ƒç”¨Claude API
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.post(
                    self.config.claude_base_url,
                    headers={
                        "x-api-key": self.config.claude_api_key,
                        "anthropic-version": "2023-06-01",
                        "content-type": "application/json"
                    },
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    raw_content = result.get('content', [{}])[0].get('text', '')
                    
                    # æ¸…ç†AIè¾“å‡ºï¼ˆå»é™¤<think>æ ‡ç­¾ç­‰ï¼‰
                    content = clean_ai_output(raw_content)
                    
                    latency = time.time() - start_time
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(provider, True, latency)
                    
                    logger.info(f"âœ… Claudeå“åº”æˆåŠŸ (è€—æ—¶: {latency:.2f}s)")
                    
                    return AIResponse(
                        success=True,
                        content=content,
                        provider=provider,
                        model=self.config.claude_model,
                        latency=latency,
                        tokens=result.get('usage', {}).get('output_tokens', 0)
                    )
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                    
        except Exception as e:
            latency = time.time() - start_time
            self._update_stats(provider, False, latency)
            
            logger.error(f"âŒ Claudeè°ƒç”¨å¤±è´¥: {e}")
            
            return AIResponse(
                success=False,
                content="",
                provider=provider,
                model=self.config.claude_model,
                latency=latency,
                error=str(e)
            )
    
    async def _get_ernie_access_token(self) -> Optional[str]:
        """
        è·å–æ–‡å¿ƒä¸€è¨€çš„access_token
        
        Returns:
            Optional[str]: access_tokenæˆ–None
        """
        try:
            token_url = "https://aip.baidubce.com/oauth/2.0/token"
            params = {
                "grant_type": "client_credentials",
                "client_id": self.config.ernie_api_key,
                "client_secret": self.config.ernie_secret_key
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.post(token_url, params=params)
                
                if response.status_code == 200:
                    result = response.json()
                    access_token = result.get('access_token')
                    logger.info("âœ… æ–‡å¿ƒä¸€è¨€access_tokenè·å–æˆåŠŸ")
                    return access_token
                else:
                    logger.error(f"âŒ è·å–æ–‡å¿ƒä¸€è¨€tokenå¤±è´¥: {response.text}")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ è·å–æ–‡å¿ƒä¸€è¨€tokenå¼‚å¸¸: {e}")
            return None
    
    async def _call_ernie(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7
    ) -> AIResponse:
        """
        è°ƒç”¨æ–‡å¿ƒä¸€è¨€API
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            AIResponse: AIå“åº”
        """
        start_time = time.time()
        provider = AIProvider.ERNIE.value
        
        try:
            if not self.ernie_available:
                raise Exception("æ–‡å¿ƒä¸€è¨€æœªé…ç½®æˆ–ä¸å¯ç”¨")
            
            # è·å–access_tokenï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not self.ernie_access_token:
                self.ernie_access_token = await self._get_ernie_access_token()
                if not self.ernie_access_token:
                    raise Exception("æ— æ³•è·å–æ–‡å¿ƒä¸€è¨€access_token")
            
            logger.info(f"ğŸ“¤ è°ƒç”¨æ–‡å¿ƒä¸€è¨€: {self.config.ernie_model}")
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "user", "content": system_prompt})
                messages.append({"role": "assistant", "content": "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚"})
            messages.append({"role": "user", "content": prompt})
            
            # æ„å»ºè¯·æ±‚ä½“
            payload = {
                "messages": messages,
                "temperature": temperature,
                "top_p": 0.8,
                "penalty_score": 1.0,
                "disable_search": False,
                "enable_citation": False
            }
            
            # API URLï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹ï¼‰
            api_url = f"{self.config.ernie_base_url}/{self.config.ernie_model}?access_token={self.ernie_access_token}"
            
            # è°ƒç”¨æ–‡å¿ƒä¸€è¨€API
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.post(
                    api_url,
                    headers={"Content-Type": "application/json"},
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                    if 'error_code' in result:
                        raise Exception(f"APIé”™è¯¯: {result.get('error_msg', 'æœªçŸ¥é”™è¯¯')}")
                    
                    raw_content = result.get('result', '')
                    
                    # æ¸…ç†AIè¾“å‡ºï¼ˆå»é™¤<think>æ ‡ç­¾ç­‰ï¼‰
                    content = clean_ai_output(raw_content)
                    
                    latency = time.time() - start_time
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self._update_stats(provider, True, latency)
                    
                    logger.info(f"âœ… æ–‡å¿ƒä¸€è¨€å“åº”æˆåŠŸ (è€—æ—¶: {latency:.2f}s)")
                    
                    return AIResponse(
                        success=True,
                        content=content,
                        provider=provider,
                        model=self.config.ernie_model,
                        latency=latency,
                        tokens=result.get('usage', {}).get('total_tokens', 0)
                    )
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                    
        except Exception as e:
            latency = time.time() - start_time
            self._update_stats(provider, False, latency)
            
            logger.error(f"âŒ æ–‡å¿ƒä¸€è¨€è°ƒç”¨å¤±è´¥: {e}")
            
            # å¦‚æœæ˜¯tokenå¤±æ•ˆï¼Œæ¸…é™¤ç¼“å­˜çš„token
            if "token" in str(e).lower() or "invalid" in str(e).lower():
                self.ernie_access_token = None
            
            return AIResponse(
                success=False,
                content="",
                provider=provider,
                model=self.config.ernie_model,
                latency=latency,
                error=str(e)
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        complexity: TaskComplexity = TaskComplexity.SIMPLE,
        temperature: float = 0.7
    ) -> AIResponse:
        """
        æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        è°ƒåº¦ç­–ç•¥ï¼š
        1. SIMPLE/MEDIUMä»»åŠ¡ -> ä¼˜å…ˆæœ¬åœ°Ollama
        2. COMPLEX/ADVANCEDä»»åŠ¡ -> ä¼˜å…ˆå…è´¹APIï¼ˆå¦‚æœé…ç½®ï¼‰
        3. å¤±è´¥è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š3æ¬¡
        4. æœ¬åœ°å¤±è´¥å¯é™çº§åˆ°å…è´¹API
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            complexity: ä»»åŠ¡å¤æ‚åº¦ï¼ˆå¯ä»¥æ˜¯TaskComplexityæšä¸¾æˆ–æ•´æ•°1-4ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            AIResponse: AIå“åº”
        """
        # å¦‚æœä¼ å…¥çš„æ˜¯æ•´æ•°ï¼Œè½¬æ¢ä¸ºTaskComplexityæšä¸¾
        if isinstance(complexity, int):
            complexity = TaskComplexity(complexity)
        
        logger.info(f"ğŸ¯ å¼€å§‹AIç”Ÿæˆä»»åŠ¡ (å¤æ‚åº¦: {complexity.name})")
        
        # æ ¹æ®å¤æ‚åº¦é€‰æ‹©æä¾›å•†
        providers = self._select_providers(complexity)
        
        # ä¾æ¬¡å°è¯•å„ä¸ªæä¾›å•†
        for attempt in range(self.config.max_retries):
            for provider in providers:
                if provider == AIProvider.OLLAMA:
                    response = await self._call_ollama(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
                    if response.success:
                        return response
                    
                    logger.warning(f"âš ï¸ {provider.value} è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†...")
                
                elif provider == AIProvider.GEMINI:
                    response = await self._call_gemini(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
                    if response.success:
                        return response
                    
                    logger.warning(f"âš ï¸ {provider.value} è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†...")
                
                elif provider == AIProvider.CLAUDE:
                    response = await self._call_claude(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
                    if response.success:
                        return response
                    
                    logger.warning(f"âš ï¸ {provider.value} è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†...")
                
                elif provider == AIProvider.ERNIE:
                    response = await self._call_ernie(
                        prompt=prompt,
                        system_prompt=system_prompt,
                        temperature=temperature
                    )
                    
                    if response.success:
                        return response
                    
                    logger.warning(f"âš ï¸ {provider.value} è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæä¾›å•†...")
                
            if attempt < self.config.max_retries - 1:
                logger.warning(f"ğŸ”„ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­...")
                await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        logger.error("âŒ æ‰€æœ‰AIæä¾›å•†è°ƒç”¨å‡å¤±è´¥")
        return AIResponse(
            success=False,
            content="",
            provider="none",
            model="none",
            latency=0.0,
            error="æ‰€æœ‰AIæä¾›å•†å‡ä¸å¯ç”¨"
        )
    
    def _select_providers(self, complexity: TaskComplexity) -> List[AIProvider]:
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦æ™ºèƒ½é€‰æ‹©AIæä¾›å•†é¡ºåºï¼ˆDay 7æ›´æ–°ï¼‰
        
        è°ƒåº¦ç­–ç•¥ï¼š
        - SIMPLE: æœ¬åœ°Ollamaï¼ˆå¿«é€Ÿã€å…è´¹ã€é€‚åˆç®€å•ä»»åŠ¡ï¼‰
        - MEDIUM: æœ¬åœ°Ollamaä¼˜å…ˆï¼Œäº‘ç«¯APIé™çº§ï¼ˆGemini/Claude/æ–‡å¿ƒä¸€è¨€ï¼‰
        - COMPLEX: é«˜çº§äº‘ç«¯APIä¼˜å…ˆï¼ˆClaude/Geminiï¼‰ï¼Œä¸­æ–‡ä»»åŠ¡ä¼˜å…ˆæ–‡å¿ƒä¸€è¨€
        - ADVANCED: ä»…é«˜çº§äº‘ç«¯APIï¼ˆClaudeä¼˜å…ˆï¼ŒGeminiæ¬¡ä¹‹ï¼‰
        
        ä¼˜å…ˆçº§ï¼ˆè´¨é‡ï¼‰ï¼š
        1. Claude-3.5-Sonnetï¼ˆæœ€å¼ºï¼‰
        2. Gemini-1.5-Flashï¼ˆå¿«é€Ÿä¸”å…è´¹ï¼‰
        3. æ–‡å¿ƒä¸€è¨€4.0ï¼ˆä¸­æ–‡ä»»åŠ¡ï¼‰
        4. Ollamaï¼ˆæœ¬åœ°å¿«é€Ÿï¼‰
        
        Args:
            complexity: ä»»åŠ¡å¤æ‚åº¦
            
        Returns:
            List[AIProvider]: æä¾›å•†åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        """
        providers = []
        
        if complexity == TaskComplexity.SIMPLE:
            # ç®€å•ä»»åŠ¡ï¼šä»…æœ¬åœ°ï¼ˆæœ€å¿«ã€æœ€çœé’±ï¼‰
            providers = [AIProvider.OLLAMA]
            
        elif complexity == TaskComplexity.MEDIUM:
            # ä¸­ç­‰ä»»åŠ¡ï¼šæœ¬åœ°ä¼˜å…ˆï¼Œäº‘ç«¯é™çº§
            if self.config.prefer_local:
                providers = [AIProvider.OLLAMA]
                # æ·»åŠ å¯ç”¨çš„äº‘ç«¯APIä½œä¸ºé™çº§
                if self.config.fallback_enabled:
                    if self.gemini_model:
                        providers.append(AIProvider.GEMINI)
                    if self.ernie_available:
                        providers.append(AIProvider.ERNIE)
                    if self.claude_available:
                        providers.append(AIProvider.CLAUDE)
            else:
                # äº‘ç«¯ä¼˜å…ˆ
                if self.gemini_model:
                    providers.append(AIProvider.GEMINI)
                if self.ernie_available:
                    providers.append(AIProvider.ERNIE)
                providers.append(AIProvider.OLLAMA)
                
        elif complexity == TaskComplexity.COMPLEX:
            # å¤æ‚ä»»åŠ¡ï¼šé«˜çº§äº‘ç«¯ä¼˜å…ˆï¼ˆè´¨é‡æ›´é‡è¦ï¼‰
            if self.config.use_cloud_for_complex:
                # æŒ‰è´¨é‡æ’åºï¼šClaude > Gemini > æ–‡å¿ƒä¸€è¨€
                if self.claude_available:
                    providers.append(AIProvider.CLAUDE)
                if self.gemini_model:
                    providers.append(AIProvider.GEMINI)
                if self.ernie_available:
                    providers.append(AIProvider.ERNIE)
                    
                # é™çº§åˆ°æœ¬åœ°
                if self.config.fallback_enabled:
                    providers.append(AIProvider.OLLAMA)
            else:
                providers = [AIProvider.OLLAMA]
                
        else:  # ADVANCED
            # é«˜çº§ä»»åŠ¡ï¼šä»…é«˜çº§äº‘ç«¯API
            if self.claude_available:
                providers = [AIProvider.CLAUDE]
            elif self.gemini_model:
                providers = [AIProvider.GEMINI]
            elif self.ernie_available:
                providers = [AIProvider.ERNIE]
            else:
                # æ‰€æœ‰äº‘ç«¯APIéƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°Ollamaå¹¶è­¦å‘Š
                logger.warning("âš ï¸ é«˜çº§ä»»åŠ¡éœ€è¦äº‘ç«¯APIï¼Œä½†å‡æœªé…ç½®ï¼Œå°†ä½¿ç”¨Ollamaï¼ˆè´¨é‡å¯èƒ½ä¸è¶³ï¼‰")
                providers = [AIProvider.OLLAMA]
        
        logger.info(f"ğŸ“‹ ä»»åŠ¡ {complexity.name} - æä¾›å•†é¡ºåº: {[p.value for p in providers]}")
        return providers
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰æä¾›å•†çš„ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.provider_stats.copy()
    
    def _update_stats(self, provider: str, success: bool, latency: float):
        """æ›´æ–°æä¾›å•†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.provider_stats[provider]
        stats['total_calls'] += 1
        
        if success:
            stats['success_calls'] += 1
        else:
            stats['failed_calls'] += 1
        
        stats['total_latency'] += latency
        stats['avg_latency'] = stats['total_latency'] / stats['total_calls']
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.provider_stats
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š AIå¼•æ“ç»Ÿè®¡ä¿¡æ¯")
        logger.info("="*60)
        
        for provider, stats in self.provider_stats.items():
            if stats['total_calls'] > 0:
                success_rate = (stats['success_calls'] / stats['total_calls']) * 100
                
                logger.info(f"\n{provider.upper()}:")
                logger.info(f"  - æ€»è°ƒç”¨: {stats['total_calls']}")
                logger.info(f"  - æˆåŠŸ: {stats['success_calls']}")
                logger.info(f"  - å¤±è´¥: {stats['failed_calls']}")
                logger.info(f"  - æˆåŠŸç‡: {success_rate:.1f}%")
                logger.info(f"  - å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.2f}s")
        
        logger.info("="*60 + "\n")


# ===== è¾…åŠ©å‡½æ•° =====

async def create_ai_engine() -> AIEngine:
    """åˆ›å»ºAIå¼•æ“å®ä¾‹"""
    config = AIConfig()
    engine = AIEngine(config)
    return engine


async def quick_generate(prompt: str, system_prompt: Optional[str] = None) -> str:
    """
    å¿«é€Ÿç”Ÿæˆæ–‡æœ¬ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
    
    Args:
        prompt: ç”¨æˆ·æç¤ºè¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        
    Returns:
        str: ç”Ÿæˆçš„æ–‡æœ¬
    """
    engine = await create_ai_engine()
    response = await engine.generate(prompt, system_prompt)
    
    if response.success:
        return response.content
    else:
        raise Exception(f"AIç”Ÿæˆå¤±è´¥: {response.error}")


# ===== ç¤ºä¾‹ç”¨æ³• =====

async def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. åˆ›å»ºAIå¼•æ“
    engine = await create_ai_engine()
    
    # 2. æµ‹è¯•è¿æ¥
    connected = await engine.test_ollama_connection()
    if not connected:
        logger.error("Ollamaè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    # 3. ç®€å•ç”Ÿæˆ
    response = await engine.generate(
        prompt="å¸®æˆ‘ä¼˜åŒ–è¿™ä¸ªé—²é±¼æ ‡é¢˜ï¼šä¹æˆæ–°iPhone 13 128G",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†æ ‡é¢˜ä¼˜åŒ–åŠ©æ‰‹ï¼Œæ“…é•¿ä¼˜åŒ–é—²é±¼å•†å“æ ‡é¢˜ã€‚",
        complexity=TaskComplexity.SIMPLE
    )
    
    if response.success:
        logger.info(f"\nâœ¨ ç”Ÿæˆç»“æœ:\n{response.content}")
    else:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {response.error}")
    
    # 4. æŸ¥çœ‹ç»Ÿè®¡
    engine.print_stats()


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(example_usage())

